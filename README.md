# Unified Explainable AI Interface

**Multi-Modal Classification System with Explainable AI**

This project integrates deepfake audio detection and lung cancer detection into a single unified interface, providing explainable AI visualizations for both modalities.

---

## ğŸ‘¥ Team Information

**TD Group:** CDOF1

**Team Members:**
- Lisa Charuel
- Aymeric Martin
- Julien De Vos

---

## ğŸ¯ Project Overview

This unified platform combines two XAI systems:

### 1. **Deepfake Audio Detection**
- **Models:** VGG16, MobileNet, ResNet, Custom CNN
- **Dataset:** Fake-or-Real (FoR) Dataset
- **Input:** `.wav` audio files
- **XAI Methods:** LIME, SHAP

### 2. **Lung Cancer Detection**
- **Models:** AlexNet, DenseNet
- **Dataset:** CheXpert chest X-rays
- **Input:** `.png`, `.jpg` image files
- **XAI Methods:** Grad-CAM, LIME, SHAP

### Key Features
âœ… Multi-modal input support (audio + images)  
âœ… Multiple pre-trained classification models  
âœ… Automatic XAI method filtering based on input type  
âœ… Side-by-side comparison of explainability techniques  
âœ… Interactive web interface built with Chainlit  

---

## ğŸ—ï¸ Project Structure

```
unified-xai-interface/
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ app.py                            # Main Chainlit application
â”œâ”€â”€ config.py                         # Configuration settings
â”‚
â”œâ”€â”€ models/                           # Model architectures & weights
â”‚   â”œâ”€â”€ audio/
â”‚   â”‚   â”œâ”€â”€ vgg16_audio.py
â”‚   â”‚   â”œâ”€â”€ mobilenet_audio.py
â”‚   â”‚   â”œâ”€â”€ resnet_audio.py
â”‚   â”‚   â””â”€â”€ custom_cnn_audio.py
â”‚   â”œâ”€â”€ image/
â”‚   â”‚   â”œâ”€â”€ alexnet_image.py
â”‚   â”‚   â””â”€â”€ densenet_image.py
â”‚   â””â”€â”€ model_loader.py               # Unified model loading
â”‚
â”œâ”€â”€ xai/                              # Explainability implementations
â”‚   â”œâ”€â”€ lime_explainer.py
â”‚   â”œâ”€â”€ shap_explainer.py
â”‚   â”œâ”€â”€ gradcam_explainer.py
â”‚   â””â”€â”€ xai_manager.py                # XAI method router
â”‚
â”œâ”€â”€ preprocessing/                     # Data preprocessing
â”‚   â”œâ”€â”€ audio_processor.py
â”‚   â””â”€â”€ image_processor.py
â”‚
â”œâ”€â”€ utils/                            # Utility functions
â”‚   â”œâ”€â”€ visualizations.py
â”‚   â”œâ”€â”€ file_handler.py
â”‚   â””â”€â”€ compatibility_checker.py
â”‚
â”œâ”€â”€ assets/                           # Static files
â”‚   â””â”€â”€ styles.css
â”‚
â”œâ”€â”€ data/                             # Sample data for testing
â”‚   â”œâ”€â”€ sample_audio/
â”‚   â””â”€â”€ sample_images/
â”‚
â”œâ”€â”€ weights/                          # Pre-trained model weights
â”‚   â”œâ”€â”€ audio_models/
â”‚   â””â”€â”€ image_models/
â”‚
â”œâ”€â”€ notebooks/                        # Jupyter notebooks for Colab
â”‚   â”œâ”€â”€ train_audio_models.ipynb
â”‚   â””â”€â”€ train_image_models.ipynb
â”‚
â””â”€â”€ docs/                            # Documentation
    â”œâ”€â”€ TECHNICAL_REPORT.md
    â”œâ”€â”€ GENERATIVE_AI_USAGE.md
    â””â”€â”€ DEMO_GUIDE.md
```

---

## ğŸš€ Installation & Setup

### Prerequisites
- Python 3.8 or higher
- pip package manager
- (Optional) GPU for faster inference - can use Google Colab

### Step 1: Clone the Repository
```bash
git clone https://github.com/lisacharuel/XAI_Final_Project.git
cd XAI_Final_Project
```

### Step 2: Create Virtual Environment
```bash
# Windows
python -m venv venv
venv\Scripts\activate

# Mac/Linux
python3 -m venv venv
source venv/bin/activate
```

### Step 3: Install Dependencies
```bash
pip install -r requirements.txt
```

### Step 4: Download Pre-trained Weights
```bash
# Run the setup script to download model weights
python setup_models.py
```

### Step 5: Prepare Sample Data
Place your test files in:
- Audio files: `data/sample_audio/`
- Image files: `data/sample_images/`

---

## ğŸ® Running the Application

### Local Deployment (Chainlit)
```bash
chainlit run app.py -w
```

Then open your browser to: `http://localhost:8000`

### Google Colab Deployment
If you want to run on Colab with GPU:
1. Open `notebooks/deploy_colab.ipynb`
2. Run all cells
3. Use the generated ngrok URL

---

## ğŸ“– User Guide

### Basic Workflow

1. **Upload Data**
   - Click "Upload File" button
   - Select audio (`.wav`) or image (`.jpg`, `.png`)

2. **Select Model**
   - Choose from available models for your data type
   - Audio: VGG16, MobileNet, ResNet, Custom CNN
   - Image: AlexNet, DenseNet

3. **Choose XAI Method**
   - Methods automatically filter based on input type
   - Audio compatible: LIME, SHAP
   - Image compatible: LIME, SHAP, Grad-CAM

4. **View Results**
   - See classification prediction
   - Explore explainability visualizations

5. **Compare (Optional)**
   - Navigate to "Comparison" tab
   - Select multiple XAI methods
   - View side-by-side analysis

---

## ğŸ”¬ XAI Methods Explained

### LIME (Local Interpretable Model-agnostic Explanations)
- **Works on:** Audio & Images
- **Explanation:** Shows which regions/features influence the prediction by perturbing input and observing changes

### SHAP (SHapley Additive exPlanations)
- **Works on:** Audio & Images
- **Explanation:** Assigns importance values to each feature based on game theory principles

### Grad-CAM (Gradient-weighted Class Activation Mapping)
- **Works on:** Images only
- **Explanation:** Highlights regions in the image that are important for the prediction using gradient information

---

## ğŸ§ª Testing

### Run Unit Tests
```bash
pytest tests/
```

### Test with Sample Data
Sample audio and images are provided in the `data/` folder for quick testing.

---

## ğŸ“Š Technical Details

### Models

**Audio Classification:**
- Input: Mel-spectrogram (128x128) from audio waveform
- Preprocessing: Librosa for audio feature extraction
- Models trained on FoR dataset (real vs. fake audio)

**Image Classification:**
- Input: Chest X-ray images (224x224)
- Preprocessing: Standard ImageNet normalization
- Models trained on CheXpert dataset (normal vs. malignant)

### Automatic Compatibility Filtering

The system automatically determines which XAI methods are compatible with the uploaded file:

```python
# Example logic
if input_type == "audio":
    available_xai = ["LIME", "SHAP"]
elif input_type == "image":
    available_xai = ["LIME", "SHAP", "Grad-CAM"]
```

---

## ğŸ“ Learning Outcomes

This project demonstrates:
- Multi-modal machine learning integration
- Explainable AI implementation across different data types
- Building user-friendly interfaces for ML models
- Model comparison and evaluation
- Software engineering best practices for ML projects

---

## ğŸ¤– Generative AI Usage Statement

### Declaration
This project utilized Generative AI tools during development.

**Tools Used:**
- Claude (Anthropic) - AI assistant

**Purposes:**
- Code architecture design and refactoring
- Documentation writing and README structure
- Debugging assistance
- XAI implementation guidance
- Best practices recommendations

**Human Contributions:**
- Model training and evaluation
- Dataset preparation and analysis
- Interface design decisions
- Testing and validation
- Final code review and modifications

All AI-generated code was reviewed, tested, and modified by team members to ensure correctness and project requirements compliance.

---

## ğŸ“ License

This project is for educational purposes as part of the XAI course curriculum.

---

## ğŸ› Known Issues & Future Improvements

### Current Limitations
- Models require significant memory for inference
- Large audio files may take time to process
- Limited to binary classification tasks

### Planned Enhancements
- [ ] Add support for CSV data
- [ ] Implement additional XAI techniques (Integrated Gradients, XRAI)
- [ ] Add interactive zoom/pan features for visualizations
- [ ] Support batch processing
- [ ] Add model performance metrics display

---



## ğŸ™ Acknowledgments

- Original Deepfake Audio Detection repository by Guri10
- Lung Cancer Detection implementation by schaudhuri16
- FoR Dataset creators
- CheXpert dataset (Stanford ML Group)
